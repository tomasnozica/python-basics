{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clase 4: Regresion Lineal Multiple\n",
    "\n",
    "La idea de regresion lineal multiple es conceptualmente igual a Regresion Lineal Simple, pero ahora la variable objetivo esta explicada por mas de una variable explicativa.\n",
    "\n",
    "Cuando tengamos dos variables explicativas, podremos graficar la relacion de las variables en un plano.\n",
    "Cuando tengamos +2 variables, no ser√° posible graficarlo.\n",
    "\n",
    "Para mas informacion, chequeamos el apunte enviado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cargamos las librerias a usar\n",
    "#Se incorpora pandas, una libreria para generar los dataset y manipularlos mejor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition / Obtencion de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cargamos boston y lo pasamos a panda\n",
    "boston_dataset = datasets.load_boston()\n",
    "boston = pd.DataFrame(boston_dataset.data, columns=boston_dataset.feature_names)\n",
    "#veamos como es la forma del dataframe\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicamos cual sera la variable objetivo\n",
    "#generamos los datos para las variables explicativas\n",
    "boston['MEDV'] = boston_dataset.target\n",
    "Y = boston['MEDV']\n",
    "#Nuestras variables explicativas sera todas menos la resultado, la medv\n",
    "X = boston.drop(columns=\"MEDV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization / Visualizacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero veamos la distribucion del precio de las casas.\n",
    "La interpretacion del grafico es: donde mas alta es la probability density, mas casas tiene un precio que ronda ese valor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 50\n",
    "fig, ax = plt.subplots()\n",
    "#Armamos el histograma\n",
    "n, bins, patches = ax.hist(Y, num_bins, density=True)\n",
    "#Seteamos el nombre de los ejes\n",
    "ax.set_xlabel('MEDV')\n",
    "ax.set_ylabel('Probability density')\n",
    "#Mostramos\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "#Otra forma interesante de visualizar son las matrices de correlacion, es decir que tanto se relacionan dos\n",
    "#varibles entre si\n",
    "#La funcion corr() crea la matriz y round(2) fija el redondeo de la correlacion\n",
    "matriz_correlacion = boston.corr().round(2)\n",
    "plt.figure(figsize=(12, 10))\n",
    "#creamos el mapa\n",
    "sns.heatmap(data=matriz_correlacion, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Algo que no esta muy bueno es la colinealidad que hay entre variables explicativas\n",
    "#Por ejemplo entre DIS e INDUS la correlacion es de | 0.71 | por lo cual nuestro modelo\n",
    "#no estaria bueno que las incluya a todas en la prediccion\n",
    "#Para los fines explicativos de esta clase, vamos a igual a hacer las 13 variables a la vez."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Training / Entrenando a los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encontramos que existe esta funcion que nos simplifica la division del dataset en train o test.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generamos el modelo y lo entrenamos\n",
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#veamos los coeficientes que definen al modelo que tiene la forma\n",
    "#Y = M1*X1 + M2*X2 + M3*X3 + ... + M13*X13\n",
    "m = regr.coef_\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realicemos una prediccion\n",
    "prediccion_precios = regr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y ahora calculemos el MSE\n",
    "mean_squared_error(Y_test, prediccion_precios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y el R2\n",
    "r2_score(Y_test,prediccion_precios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
